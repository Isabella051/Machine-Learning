{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8945c17b-8e0c-4575-a18e-32a771e64b2f",
   "metadata": {},
   "source": [
    "# A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2125873-1c3f-47c0-be67-00efb7c8c92b",
   "metadata": {},
   "source": [
    "In this assignment, I first implemented a Multinomial Naive Bayes classifier from scratch using user review texts as input. After filling missing values, converting all text to lowercase, and removing stop words, I constructed a bag‑of‑words count matrix. By computing each class’s prior probabilities and each term’s conditional probabilities—with Laplace smoothing and logarithmic scaling—I built the baseline model. On the test set, this model achieved an accuracy of 0.8963 (about 0.90 overall); specifically, for the Nightlife category precision, recall, and F1‑score were all 0.61, for Restaurants they were 0.92/0.93/0.93, and for Shopping they were 0.93/0.92/0.92, yielding a weighted average accuracy of 0.90.\n",
    "\n",
    "Next, inspired by Rennie et al. (2003) “Tackling the Poor Assumptions of Naive Bayes Text Classifiers,” I replaced raw term counts with TF‑IDF features and switched to the ComplementNB classifier to correct Naive Bayes’s weaknesses under class imbalance and extreme word‑frequency distributions. Using stratified five‑fold cross‑validation and grid search to tune sublinear\\_tf, use\\_idf, norm, min\\_df, ngram\\_range, and the smoothing parameter alpha, the model reached a best CV accuracy of 0.8867 and improved to 0.9048 on the test set. In that configuration, Nightlife achieved precision/recall/F1 of 0.71/0.64/0.67, Restaurants 0.94/0.93/0.93, and Shopping 0.90/0.94/0.92.\n",
    "\n",
    "Finally, when exploring additional attributes to improve performance, I found that ID, latitude/longitude, and mean\\_checkin\\_time bore little relation to review categories. Instead, I introduced the “name” field as an extra textual feature—independent of the words in category‑specific reviews—and used a ColumnTransformer to apply customized TF‑IDF separately to reviews and to names. By merging these two feature streams in a single Pipeline and jointly grid‑searching both streams’ TF‑IDF parameters along with the classifier’s smoothing parameter, the multi‑attribute fusion model achieved a CV accuracy of 0.9023 and a test accuracy of 0.9162. In that final model, Nightlife’s precision/recall/F1 rose to 0.74/0.66/0.69, Restaurants to 0.94/0.94/0.94, and Shopping to 0.91/0.95/0.93—demonstrating that each iterative enhancement steadily increased the model’s predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018474c-26aa-45d7-bd07-32c9e6757df1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 1. Build a baseline model by implementing the Naive Bayes classifier from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225c45e4-b021-4475-9d9b-fa8d19d339a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load training and testing datasets from CSV files\n",
    "train_df = pd.read_csv(\"training.csv\")\n",
    "test_df  = pd.read_csv(\"testing.csv\")\n",
    "# Extract the 'review' column as input features, replacing any missing values with empty strings\n",
    "X_train = train_df[\"review\"].fillna(\"\")\n",
    "X_test  = test_df[\"review\"].fillna(\"\")\n",
    "# Extract the 'category' column as target labels and convert to a NumPy array\n",
    "y_train = train_df[\"category\"].to_numpy()\n",
    "y_test = test_df[\"category\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2af9ba1-2bb5-4eaa-a089-ba1a153604ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self, alpha: float = 1.0):\n",
    "        self.alpha = alpha # Laplace smoothing constant\n",
    "        self.classes_ = None # Unique class labels\n",
    "        self.class_log_prior_ = None # Log prior probabilities log(P(c))\n",
    "        self.feature_log_prob_ = None # Log conditional probabilities log(P(w_j | c))\n",
    "        self.vocab_size_ = None # Number of features (vocabulary size)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Identify the unique classes and map y to integer indices\n",
    "        self.classes_, y_idx = np.unique(y, return_inverse=True)\n",
    "        n_classes, n_features = len(self.classes_), X.shape[1]\n",
    "        self.vocab_size_ = n_features\n",
    "\n",
    "        # Count how many samples belong to each class\n",
    "        class_counts = np.bincount(y_idx)\n",
    "        # Compute log prior probabilities: log(P(c)) = log(n_c) - log(n_total)\n",
    "        self.class_log_prior_ = np.log(class_counts) - np.log(class_counts.sum())\n",
    "\n",
    "        # Initialize a matrix to count feature occurrences per class\n",
    "        feature_count = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        # Sum up the counts of each feature for samples in each class\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            feature_count[i, :] = X[y == c].sum(axis=0)\n",
    "\n",
    "        # Apply Laplace smoothing to avoid zero probabilities\n",
    "        smoothed = feature_count + self.alpha\n",
    "        # Compute the normalization denominator for each class (total count of words + alpha * V)\n",
    "        denom = smoothed.sum(axis=1, keepdims=True)\n",
    "        # Compute log probabilities of features given class: log(P(w_j | c))\n",
    "        self.feature_log_prob_ = np.log(smoothed) - np.log(denom)\n",
    "        return self\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        # jll = X * log(P(w|c)).T + log(P(c))\n",
    "        return X @ self.feature_log_prob_.T + self.class_log_prior_\n",
    "\n",
    "    def predict(self, X):\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        # Select the class with the highest joint log likelihood\n",
    "        return self.classes_[np.argmax(jll, axis=1)]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        # Compute log of marginal likelihood: log P(x) = logsumexp over classes\n",
    "        log_prob_x = np.logaddexp.reduce(jll, axis=1, keepdims=True)\n",
    "        # Convert joint log likelihoods to normalized probabilities\n",
    "        return np.exp(jll - log_prob_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f5305c-d602-4b41-88b8-7f0a0e5b9336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of baseline model: 0.8963\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Nightlife       0.61      0.61      0.61        64\n",
      " Restaurants       0.92      0.93      0.93       422\n",
      "    Shopping       0.93      0.92      0.92       218\n",
      "\n",
      "    accuracy                           0.90       704\n",
      "   macro avg       0.82      0.82      0.82       704\n",
      "weighted avg       0.90      0.90      0.90       704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a CountVectorizer to transform text into token counts\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True, # convert all characters to lowercase before tokenizing\n",
    "    stop_words=\"english\",  # remove common English stop words\n",
    "    min_df=2 # ignore terms that appear in fewer than 2 documents              \n",
    ")\n",
    "\n",
    "# Learn the vocabulary from the training data and vectorize the text\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "# Transform the test data using the same vocabulary\n",
    "X_test  = vectorizer.transform(X_test)\n",
    "\n",
    "# Create and train the Multinomial Naive Bayes classifier\n",
    "nb = MultinomialNaiveBayes(alpha=1.0)\n",
    "nb.fit(X_train, y_train);\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = nb.predict(X_test)\n",
    "# Evaluate and print the model's accuracy\n",
    "print(f\"Accuracy of baseline model: {accuracy_score(y_test, y_pred):.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ead5f-73bc-4340-be34-687268e34da6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2. Improve on the benchmark model based on the review attribute only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd25ea91-030d-464a-bd78-fb3df18005fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "Best CV Accuracy: 0.8867\n",
      "Best hyperparameters: {'complementnb__alpha': 0.95, 'tfidfvectorizer__min_df': 2, 'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__norm': None, 'tfidfvectorizer__sublinear_tf': True, 'tfidfvectorizer__use_idf': False}\n",
      "Accuracy  : 0.9048\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Nightlife       0.71      0.64      0.67        64\n",
      " Restaurants       0.94      0.93      0.93       422\n",
      "    Shopping       0.90      0.94      0.92       218\n",
      "\n",
      "    accuracy                           0.90       704\n",
      "   macro avg       0.85      0.84      0.84       704\n",
      "weighted avg       0.90      0.90      0.90       704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Prepare text data: fill missing reviews with empty strings\n",
    "X_train = train_df[\"review\"].fillna(\"\")\n",
    "y_train = train_df[\"category\"].to_numpy()\n",
    "X_test  = test_df[\"review\"].fillna(\"\")\n",
    "y_test = test_df[\"category\"].to_numpy()\n",
    "# Build a pipeline: first compute TF-IDF features, then apply ComplementNB classifier\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(lowercase=True, stop_words=\"english\"), # convert all text to lowercase and remove common English stop words\n",
    "    ComplementNB() # initialize the Complement Naive Bayes classifier\n",
    ")\n",
    "# Define hyperparameter grid for exhaustive search\n",
    "param_grid = {\n",
    "    \"tfidfvectorizer__sublinear_tf\": [True, False],\n",
    "    \"tfidfvectorizer__use_idf\":      [True, False],\n",
    "    \"tfidfvectorizer__norm\":         [\"l1\", \"l2\", None],\n",
    "    \"tfidfvectorizer__min_df\":       [2, 3, 4],\n",
    "    \"tfidfvectorizer__ngram_range\":  [(1, 1), (1, 2)],\n",
    "    \"complementnb__alpha\":           [0.9, 0.95, 1.0]\n",
    "}\n",
    "# Set up stratified 5‑fold cross‐validation to preserve class proportions in each fold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"Best CV Accuracy: {grid_search.best_score_:.4f}\")\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Accuracy  : {:.4f}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a017a68b-f3ad-4e67-abde-2f142b224a62",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3. Improve your model by adding additional attributes to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fdf4ba4-57d5-477c-a97f-854acfb3bef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "Best CV Accuracy: 0.9023\n",
      "Best Hyperparameters:\n",
      "  - clf__alpha: 0.95\n",
      "  - features__name_tfidf__max_features: 500\n",
      "  - features__name_tfidf__ngram_range: (1, 2)\n",
      "  - features__name_tfidf__norm: None\n",
      "  - features__name_tfidf__sublinear_tf: True\n",
      "  - features__name_tfidf__use_idf: False\n",
      "  - features__review_tfidf__min_df: 2\n",
      "  - features__review_tfidf__ngram_range: (1, 2)\n",
      "  - features__review_tfidf__norm: None\n",
      "  - features__review_tfidf__sublinear_tf: True\n",
      "  - features__review_tfidf__use_idf: False\n",
      "\n",
      "Accuracy: 0.9162\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Nightlife       0.74      0.66      0.69        64\n",
      " Restaurants       0.94      0.94      0.94       422\n",
      "    Shopping       0.91      0.95      0.93       218\n",
      "\n",
      "    accuracy                           0.92       704\n",
      "   macro avg       0.86      0.85      0.86       704\n",
      "weighted avg       0.91      0.92      0.92       704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Fill missing product names with empty string so vectorizer can handle them\n",
    "train_df[\"name\"]   = train_df[\"name\"].fillna(\"\")\n",
    "test_df[\"name\"]   = test_df[\"name\"].fillna(\"\")\n",
    "X_train = train_df[[\"review\", \"name\"]]\n",
    "X_test  = test_df[[\"review\", \"name\"]]\n",
    "# Define a ColumnTransformer \n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"review_tfidf\", TfidfVectorizer(lowercase=True, stop_words=\"english\"),\n",
    "     \"review\"),\n",
    "    (\"name_tfidf\",   TfidfVectorizer(lowercase=True, analyzer=\"word\"),\n",
    "     \"name\")\n",
    "])\n",
    "\n",
    "# Build a pipeline that first transforms features using our preprocessor,\n",
    "# then fits a Complement Naive Bayes classifier on the combined TF-IDF features\n",
    "pipeline = Pipeline([\n",
    "    (\"features\", preprocessor),\n",
    "    (\"clf\", ComplementNB())\n",
    "])\n",
    "\n",
    "# Specify grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    # Hyperparameters for the review TF-IDF transformer\n",
    "    \"features__review_tfidf__sublinear_tf\": [True],\n",
    "    \"features__review_tfidf__use_idf\": [False],\n",
    "    \"features__review_tfidf__norm\": [None],\n",
    "    \"features__review_tfidf__ngram_range\": [(1,2)],\n",
    "    \"features__review_tfidf__min_df\": [2],\n",
    "    # Hyperparameters for the name TF-IDF transformer\n",
    "    \"features__name_tfidf__sublinear_tf\": [True, False],\n",
    "    \"features__name_tfidf__use_idf\": [True, False],\n",
    "    \"features__name_tfidf__norm\": [\"l2\", \"l2\", None],\n",
    "    \"features__name_tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"features__name_tfidf__max_features\": [500, 1000],\n",
    "    # Smoothing parameter for the ComplementNB classifier\n",
    "    \"clf__alpha\": [0.8, 0.95, 1.0]\n",
    "}\n",
    "# Use stratified 5‑fold cross‑validation to preserve class distribution in each fold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, param_grid,\n",
    "    cv=cv, scoring=\"accuracy\",\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print final accuracy on test data\n",
    "print(f\"Best CV Accuracy: {grid_search.best_score_:.4f}\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, val in grid_search.best_params_.items():\n",
    "    print(f\"  - {param}: {val}\")\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
