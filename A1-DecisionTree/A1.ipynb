{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f89cbe-5fdb-4df1-a61d-ab38b9ae1024",
   "metadata": {
    "panel-layout": {
     "height": 50.8203125,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# COMPSCI 762 Assignment1\n",
    "# Task 1\n",
    "## 1.1 Load the datasets using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3edc7f8a-05a9-47a3-b9ae-e4e0ee786986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset \n",
    "letter_recognition = fetch_ucirepo(id=59)  \n",
    "adult = fetch_ucirepo(id=2)\n",
    "mushroom = fetch_ucirepo(id=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc5c4866-7e4e-4f17-845e-fab62743e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data (as pandas dataframes) \n",
    "letter_X = letter_recognition.data.features \n",
    "letter_y = letter_recognition.data.targets  \n",
    "adult_X = adult.data.features \n",
    "adult_y = adult.data.targets \n",
    "mushroom_X = mushroom.data.features \n",
    "mushroom_y = mushroom.data.targets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40788576-33b1-4330-ad94-15a471d4687f",
   "metadata": {},
   "source": [
    "## 1.2 Check for missing values and handle them appropriately.\n",
    "No missing value for letter_recognition data. There's some missing values for adult and adult data. Then replace them with the mode since they are categorical variables to keep the balance of the distribution of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7b46eb-e314-4541-829d-7c38dbe2d131",
   "metadata": {
    "panel-layout": {
     "height": 0,
     "visible": true,
     "width": 100
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/4097588802.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  adult_X[\"workclass\"].fillna(mode_val, inplace=True)\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/4097588802.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  adult_X[\"workclass\"].fillna(mode_val, inplace=True)\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/4097588802.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  adult_X[\"occupation\"].fillna(mode_val, inplace=True)\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/4097588802.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  adult_X[\"occupation\"].fillna(mode_val, inplace=True)\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/4097588802.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  adult_X[\"native-country\"].fillna(mode_val, inplace=True)\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/4097588802.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  adult_X[\"native-country\"].fillna(mode_val, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "mode_val = adult_X[\"workclass\"].mode()[0]\n",
    "adult_X[\"workclass\"].fillna(mode_val, inplace=True)\n",
    "mode_val = adult_X[\"occupation\"].mode()[0]\n",
    "adult_X[\"occupation\"].fillna(mode_val, inplace=True)\n",
    "mode_val = adult_X[\"native-country\"].mode()[0]\n",
    "adult_X[\"native-country\"].fillna(mode_val, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b3cd78-18bf-49a2-a36b-94906b5ddf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education-num     0\n",
       "marital-status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital-gain      0\n",
       "capital-loss      0\n",
       "hours-per-week    0\n",
       "native-country    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eca68ad-083d-448d-9d57-e538b5b45967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/2310256817.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  mushroom_X[\"stalk-root\"].fillna(mode_val, inplace=True)\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/2310256817.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mushroom_X[\"stalk-root\"].fillna(mode_val, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "mode_val = mushroom_X[\"stalk-root\"].mode()[0]\n",
    "mushroom_X[\"stalk-root\"].fillna(mode_val, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38e6ff63-07b6-4f61-b970-bca478d8693a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cap-shape                   0\n",
       "cap-surface                 0\n",
       "cap-color                   0\n",
       "bruises                     0\n",
       "odor                        0\n",
       "gill-attachment             0\n",
       "gill-spacing                0\n",
       "gill-size                   0\n",
       "gill-color                  0\n",
       "stalk-shape                 0\n",
       "stalk-root                  0\n",
       "stalk-surface-above-ring    0\n",
       "stalk-surface-below-ring    0\n",
       "stalk-color-above-ring      0\n",
       "stalk-color-below-ring      0\n",
       "veil-type                   0\n",
       "veil-color                  0\n",
       "ring-number                 0\n",
       "ring-type                   0\n",
       "spore-print-color           0\n",
       "population                  0\n",
       "habitat                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mushroom_X.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed39b7-fa03-4507-8429-88a775632d9b",
   "metadata": {},
   "source": [
    "There's no missing values after imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b644c-9242-48f8-aa37-6df51bbef883",
   "metadata": {
    "panel-layout": {
     "height": 50.8203125,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# Task 2\n",
    "## 2.1 Decision Stump – A single-split decision tree (i.e. depth = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0b246ea-1a8b-4366-9eb9-249b3551a503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n",
      "/var/folders/f6/yfhszs1j2591wk2769lbsv140000gn/T/ipykernel_13760/532627664.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df[col] = le.fit_transform(X_df[col])\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the datasets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_categorical(X_df, y_df):\n",
    "    # Convert categorical features to numercial values for modelling\n",
    "    categorical_cols = X_df.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X_df[col] = le.fit_transform(X_df[col])\n",
    "\n",
    "    # Convert target values to numercial values for modelling (all target values are categorical)\n",
    "    le_y = LabelEncoder()\n",
    "    y_np = le_y.fit_transform(y_df.to_numpy().ravel())\n",
    "\n",
    "    # convert DataFrame into NumPy array\n",
    "    X_np = X_df.values\n",
    "    return X_np, y_np\n",
    "\n",
    "## get the numerical Numpy array\n",
    "X_np_letter, y_np_letter = encode_categorical(letter_X, letter_y)\n",
    "X_np_mushroom, y_np_mushroom = encode_categorical(mushroom_X, mushroom_y)\n",
    "adult_y = adult_y.replace({\n",
    "    '<=50K.': '<=50K',\n",
    "    '>50K.': '>50K'\n",
    "}) ## fix the incorrect values\n",
    "X_np_adult, y_np_adult = encode_categorical(adult_X, adult_y)\n",
    "\n",
    "## Split three datasets into 80% training set and 20% test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "\n",
    "## get the training and testing datasets for modelling\n",
    "letter_X_train, letter_X_test, letter_y_train, letter_y_test = split_dataset(X_np_letter, y_np_letter, test_size=0.2, random_state=42)\n",
    "adult_X_train, adult_X_test, adult_y_train, adult_y_test = split_dataset(X_np_adult, y_np_adult, test_size=0.2, random_state=42)\n",
    "mushroom_X_train, mushroom_X_test, mushroom_y_train, mushroom_y_test = split_dataset(X_np_mushroom, y_np_mushroom, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1782c0f8-bf71-4d1d-a7a6-9bf4d14de176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(counts):\n",
    "    total = np.sum(counts)\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    probs = counts / total\n",
    "    return -np.sum(probs * np.log2(probs + 1e-12))  # prevent log2(0)\n",
    "\n",
    "def infor_gain(total_counts, left_counts):\n",
    "    n_total = np.sum(total_counts)\n",
    "    n_left = np.sum(left_counts)\n",
    "    n_right = n_total - n_left\n",
    "    right_counts = total_counts - left_counts\n",
    "\n",
    "    # Compute entropy for each set\n",
    "    ent_parent = entropy(total_counts)\n",
    "    ent_left = entropy(left_counts)\n",
    "    ent_right = entropy(right_counts)\n",
    "    \n",
    "    # Compute information gain\n",
    "    weighted_sum = (n_left / n_total) * ent_left + (n_right / n_total) * ent_right\n",
    "    return ent_parent - weighted_sum\n",
    "\n",
    "def find_best_split_for_feature(X_col, y, num_classes):\n",
    "    # Sort by feature values\n",
    "    sort_idx = np.argsort(X_col)\n",
    "    X_sorted = X_col[sort_idx]\n",
    "    y_sorted = y[sort_idx]\n",
    "    \n",
    "    n_samples = len(X_col)\n",
    "\n",
    "    # Build prefix counts: record how many times each class appears in the first i samples (after sorting by feature value).\n",
    "    # This allows fast calculation of the label distribution in the left and right subsets to compute information gain,\n",
    "    # and directly obtain the label counts in the subsets, reducing runtime.\n",
    "    # Initialize a NumPy array.\n",
    "    prefix_counts = np.zeros((n_samples + 1, num_classes), dtype=np.int64)\n",
    "    \n",
    "    # Map all values in y to indices [0..num_classes-1]\n",
    "    for i in range(n_samples):\n",
    "        c = y_sorted[i]\n",
    "        prefix_counts[i+1] = prefix_counts[i]\n",
    "        prefix_counts[i+1, c] += 1\n",
    "\n",
    "    # Total label distribution for the parent node\n",
    "    total_counts = prefix_counts[n_samples]\n",
    "    best_ig = -1.0\n",
    "    best_threshold = None\n",
    "    best_left_label = None\n",
    "    best_right_label = None\n",
    "    n_samples = len(X_sorted)\n",
    "\n",
    "    for i in range(n_samples - 1):\n",
    "        # Skip duplicate threshold values\n",
    "        if X_sorted[i] != X_sorted[i + 1]:\n",
    "            threshold = X_sorted[i]\n",
    "\n",
    "            # Left subset = indices [0..i] with its label distribution:\n",
    "            left_counts = prefix_counts[i+1]\n",
    "            right_counts = total_counts - left_counts\n",
    "\n",
    "            # Avoid empty left/right subsets\n",
    "            n_left = np.sum(left_counts)\n",
    "            n_right = np.sum(right_counts)\n",
    "            if n_left == 0 or n_right == 0:\n",
    "                continue\n",
    "\n",
    "            ig = infor_gain(total_counts, left_counts)\n",
    "            if ig > best_ig:\n",
    "                best_ig = ig\n",
    "                best_threshold = threshold\n",
    "                best_left_label = np.argmax(left_counts)\n",
    "                best_right_label = np.argmax(right_counts)\n",
    "\n",
    "    return best_ig, best_threshold, best_left_label, best_right_label\n",
    "\n",
    "class DecisionStump:\n",
    "    def __init__(self, column_names=None):\n",
    "        self.feature_idx = None\n",
    "        self.threshold = None\n",
    "        self.left_label = None\n",
    "        self.right_label = None\n",
    "        self.ig = None\n",
    "        self.column_names = column_names\n",
    "    \n",
    "    def fit(self, X, y, n_jobs=1):\n",
    "        # Determine the number of classes\n",
    "        unique_labels = np.unique(y)\n",
    "        num_classes = len(unique_labels)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        results = []\n",
    "        for f in range(n_features):\n",
    "            best_ig, best_threshold, left_label, right_label = find_best_split_for_feature(\n",
    "                X[:, f], y, num_classes\n",
    "            )\n",
    "            results.append((best_ig, best_threshold, left_label, right_label))\n",
    "\n",
    "        best_ig_global = -1.0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_left_label = None\n",
    "        best_right_label = None\n",
    "        \n",
    "        for f_idx, (ig, thr, left_lbl, right_lbl) in enumerate(results):\n",
    "            if ig > best_ig_global:\n",
    "                best_ig_global = ig\n",
    "                best_feature = f_idx\n",
    "                best_threshold = thr\n",
    "                best_left_label = left_lbl\n",
    "                best_right_label = right_lbl\n",
    "        \n",
    "        # Save to the stump's attributes\n",
    "        self.feature_idx = best_feature\n",
    "        self.threshold = best_threshold\n",
    "        self.left_label = best_left_label\n",
    "        self.right_label = best_right_label\n",
    "        self.ig = best_ig_global\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for sample in X:\n",
    "            if sample[self.feature_idx] <= self.threshold:\n",
    "                preds.append(self.left_label)\n",
    "            else:\n",
    "                preds.append(self.right_label)\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def print_stump_details(self):\n",
    "        feature_name = self.column_names[self.feature_idx]\n",
    "        print(f\"The most important feature for the stump model: {feature_name}, with information gain: {self.ig}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9056a27b-09a0-4cd9-9b33-3e81830118f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train letter dataset\n",
    "import time\n",
    "start = time.time()\n",
    "stump_letter = DecisionStump(column_names=letter_X.columns.tolist())\n",
    "stump_letter.fit(letter_X_train, letter_y_train)\n",
    "end = time.time()\n",
    "letter_runtime1 = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "993301a8-64e2-44fe-b729-51e67aa9309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train adult dataset\n",
    "start = time.time()\n",
    "stump_adult = DecisionStump(column_names=adult_X.columns.tolist())\n",
    "stump_adult.fit(adult_X_train, adult_y_train)\n",
    "end = time.time()\n",
    "adult_runtime1 = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "602ec8b4-91be-46c5-bbd3-bc43eb669e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train mushroom dataset\n",
    "start = time.time()\n",
    "stump_mushroom = DecisionStump(column_names=mushroom_X.columns.tolist())\n",
    "stump_mushroom.fit(mushroom_X_train, mushroom_y_train)\n",
    "end = time.time()\n",
    "mushroom_runtime1 = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b864b4-5df8-43a6-918f-ca66f5e85e59",
   "metadata": {
    "panel-layout": {
     "height": 50.8203125,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## 2.2. Unpruned Decision Tree  – A full-depth decision tree grown without constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f4632da-e62e-4ba0-8a3c-339210bd3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(X, y, num_classes):\n",
    "    # Find the best split across all features\n",
    "    n_samples, n_features = X.shape\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_left_label = None\n",
    "    best_right_label = None\n",
    "    best_ig = -1.0\n",
    "\n",
    "    for f in range(n_features):\n",
    "        ig, thr, left_lbl, right_lbl = find_best_split_for_feature(X[:, f], y, num_classes)\n",
    "        if ig > best_ig:\n",
    "            best_ig = ig\n",
    "            best_feature = f\n",
    "            best_threshold = thr\n",
    "            best_left_label = left_lbl\n",
    "            best_right_label = right_lbl\n",
    "\n",
    "    return best_feature, best_threshold, best_left_label, best_right_label, best_ig\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, is_leaf=False, label=None,\n",
    "                 feature_idx=None, threshold=None,\n",
    "                 left_child=None, right_child=None):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.label = label\n",
    "        self.feature_idx = feature_idx\n",
    "        self.threshold = threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.ig = None\n",
    "\n",
    "class DecisionTreeUnpruned:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def build_tree(self, X, y, num_classes):\n",
    "        # If all samples belong to the same class, return a leaf node\n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "        if len(unique_labels) == 1:\n",
    "            return TreeNode(is_leaf=True, label=unique_labels[0])\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_threshold, left_label, right_label, best_ig = find_best_split(X, y, num_classes)\n",
    "\n",
    "        # If information gain is <= 0, further splitting is not possible; set as a leaf node (using majority voting)\n",
    "        if best_ig <= 1e-12:\n",
    "            majority_label = unique_labels[counts.argmax()]  # Label with the highest frequency\n",
    "            return TreeNode(is_leaf=True, label=majority_label)\n",
    "\n",
    "        # Split data using best_feature and best_threshold\n",
    "        left_mask = (X[:, best_feature] <= best_threshold)\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        X_left, y_left = X[left_mask], y[left_mask]\n",
    "        X_right, y_right = X[right_mask], y[right_mask]\n",
    "\n",
    "        # If one side is empty, further splitting is not possible; use majority voting\n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            majority_label = unique_labels[counts.argmax()]\n",
    "            return TreeNode(is_leaf=True, label=majority_label)\n",
    "\n",
    "        # Recursively build the left and right subtrees\n",
    "        left_child = self.build_tree(X_left, y_left, num_classes)\n",
    "        right_child = self.build_tree(X_right, y_right, num_classes)\n",
    "\n",
    "        # Create the current node\n",
    "        node = TreeNode(is_leaf=False,\n",
    "                        feature_idx=best_feature,\n",
    "                        threshold=best_threshold,\n",
    "                        left_child=left_child,\n",
    "                        right_child=right_child)\n",
    "        node.ig = best_ig\n",
    "        return node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Calculate the total number of classes\n",
    "        global_num_classes = len(np.unique(y))\n",
    "        self.root = self.build_tree(X, y, global_num_classes)\n",
    "\n",
    "    def predict_sample(self, x, node):\n",
    "        # Recursively make a prediction for a single sample x starting from the given node\n",
    "        if node.is_leaf:\n",
    "            return node.label\n",
    "        # Traverse left or right according to the splitting rule\n",
    "        if x[node.feature_idx] <= node.threshold:\n",
    "            return self.predict_sample(x, node.left_child)\n",
    "        else:\n",
    "            return self.predict_sample(x, node.right_child)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for i in range(len(X)):\n",
    "            pred_label = self.predict_sample(X[i], self.root)\n",
    "            preds.append(pred_label)\n",
    "        return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea563f3a-e81f-4ace-9313-9b8901c782a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train on letter dataset\n",
    "start = time.time()\n",
    "letter_UnprunedTree = DecisionTreeUnpruned()\n",
    "letter_UnprunedTree.fit(letter_X_train, letter_y_train)\n",
    "end = time.time()\n",
    "letter_runtime2 = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c45b79d3-df2e-4f3d-8872-9472c736967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train on adult dataset\n",
    "start = time.time()\n",
    "adult_UnprunedTree = DecisionTreeUnpruned()\n",
    "adult_UnprunedTree.fit(adult_X_train, adult_y_train)\n",
    "end = time.time()\n",
    "adult_runtime2 = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "519c1be7-0d65-4ca8-9115-c50c39151b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train on mushroom dataset\n",
    "start = time.time()\n",
    "mushroom_UnprunedTree = DecisionTreeUnpruned()\n",
    "mushroom_UnprunedTree.fit(mushroom_X_train, mushroom_y_train)\n",
    "end = time.time()\n",
    "mushroom_runtime2 = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a9522d-7ee6-4878-a056-cfc2bc84da16",
   "metadata": {
    "panel-layout": {
     "height": 50.8203125,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## 2.3 Pruned Decision Tree – Implement pruning to prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4923b0f9-e2ea-4e24-a5ef-5c41e938fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreePrePruned:\n",
    "    # Decision tree with pre-pruning (limits: max_depth, min_samples_leaf, min_info_gain).\n",
    "    def __init__(self, max_depth=None, min_samples_leaf=1, min_info_gain=1e-12):\n",
    "        self.max_depth = max_depth  # Maximum allowed depth of the tree (starting from 0)\n",
    "        self.min_samples_leaf = min_samples_leaf  # Minimum number of samples required in a leaf node\n",
    "        self.min_info_gain = min_info_gain  # Minimum information gain threshold for a split\n",
    "        self.root = None\n",
    "\n",
    "    def build_tree(self, X, y, num_classes, depth=0):\n",
    "        # Recursively build the subtree and return its root node\n",
    "        n_samples = len(y)\n",
    "        unique_labels, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "        # If all sample labels are the same -> create a leaf node\n",
    "        if len(unique_labels) == 1:\n",
    "            return TreeNode(is_leaf=True, label=unique_labels[0])\n",
    "\n",
    "        # If too few samples or reached max_depth -> create a leaf node (using majority vote)\n",
    "        if n_samples <= self.min_samples_leaf:\n",
    "            majority_label = unique_labels[np.argmax(counts)]\n",
    "            return TreeNode(is_leaf=True, label=majority_label)\n",
    "\n",
    "        if (self.max_depth is not None) and (depth >= self.max_depth):\n",
    "            # Maximum depth reached\n",
    "            majority_label = unique_labels[np.argmax(counts)]\n",
    "            return TreeNode(is_leaf=True, label=majority_label)\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_threshold, left_label, right_label, best_ig = find_best_split(X, y, num_classes)\n",
    "\n",
    "        # If information gain is less than min_info_gain -> create a leaf node\n",
    "        if best_ig < self.min_info_gain:\n",
    "            majority_label = unique_labels[np.argmax(counts)]\n",
    "            return TreeNode(is_leaf=True, label=majority_label)\n",
    "\n",
    "        # Split the data\n",
    "        left_mask = (X[:, best_feature] <= best_threshold)\n",
    "        right_mask = ~left_mask\n",
    "        X_left, y_left = X[left_mask], y[left_mask]\n",
    "        X_right, y_right = X[right_mask], y[right_mask]\n",
    "\n",
    "        # If either subset has fewer samples than min_samples_leaf, do not split and create a leaf node\n",
    "        if (len(y_left) < self.min_samples_leaf) or (len(y_right) < self.min_samples_leaf):\n",
    "            majority_label = unique_labels[np.argmax(counts)]\n",
    "            return TreeNode(is_leaf=True, label=majority_label)\n",
    "\n",
    "        # Recursively build left and right subtrees\n",
    "        left_child = self.build_tree(X_left, y_left, num_classes, depth + 1)\n",
    "        right_child = self.build_tree(X_right, y_right, num_classes, depth + 1)\n",
    "\n",
    "        # Create a non-leaf node\n",
    "        node = TreeNode(is_leaf=False,\n",
    "                        feature_idx=best_feature,\n",
    "                        threshold=best_threshold,\n",
    "                        left_child=left_child,\n",
    "                        right_child=right_child)\n",
    "        node.ig = best_ig \n",
    "        return node\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Calculate the total number of classes\n",
    "        global_num_classes = len(np.unique(y))\n",
    "        self.root = self.build_tree(X, y, global_num_classes, depth=0)\n",
    "\n",
    "    def predict_sample(self, x, node):\n",
    "        # Recursively predict the label for a single sample x starting from node\n",
    "        if node.is_leaf:\n",
    "            return node.label\n",
    "        if x[node.feature_idx] <= node.threshold:\n",
    "            return self.predict_sample(x, node.left_child)\n",
    "        else:\n",
    "            return self.predict_sample(x, node.right_child)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for i in range(len(X)):\n",
    "            preds.append(self.predict_sample(X[i], self.root))\n",
    "        return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eaa8f0b-bdbc-4449-ae48-ff76119e1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train on letter dataset\n",
    "start = time.time()\n",
    "letter_PrunedTree = DecisionTreePrePruned()\n",
    "letter_PrunedTree.fit(letter_X_train, letter_y_train)\n",
    "end = time.time()\n",
    "letter_runtime3 = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4dd42dd-5d63-46f7-b66e-ebc9982bb1f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## train on adult dataset\n",
    "start = time.time()\n",
    "adult_PrunedTree = DecisionTreePrePruned()\n",
    "adult_PrunedTree.fit(adult_X_train, adult_y_train)\n",
    "end = time.time()\n",
    "adult_runtime3 = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67927aba-2e26-4eb4-b602-823e4cc77ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train mushroom dataset\n",
    "start = time.time()\n",
    "mushroom_PrunedTree = DecisionTreePrePruned()\n",
    "mushroom_PrunedTree.fit(mushroom_X_train, mushroom_y_train)\n",
    "end = time.time()\n",
    "mushroom_runtime3 = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd178ac-43ee-40d0-8305-eede16ec882f",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "## 3.1 Select optimal hyperparameters (e.g. tree depth, minimum samples per split).\n",
    "### Describe your selection method (e.g. cross-validation, validation set).\n",
    "Stratified 5-Fold Cross-Validation is used to evaluate the performance of different hyperparameter combinations. The training set was divided into 5 subsets, while ensuring that the class distribution in each subset remains consistent with the overall dataset. For each set of hyperparameters (including tree depth, minimum samples per leaf, and minimum information gain), the model is trained on 4 folds and validated on the remaining fold. The F1 scores from all folds are averaged to serve as the final evaluation metric for that set of hyperparameters. The code iterates over a predefined grid of hyperparameters, calculates the average F1 score for each combination, and selects the hyperparameter combination that performs the best.\n",
    "\n",
    "### Explain how different hyperparameter choices impact performance.\n",
    "A deeper tree can capture more complex patterns but is prone to overfitting, where the model performs well on training data but poorly on unseen data. A shallower tree may underfit if it cannot capture the necessary patterns in the data.\n",
    "\n",
    "Setting a higher minimum number of samples required at a leaf helps prevent the model from creating leaves that represent only a few samples, thereby reducing overfitting and improving generalization. However, if set too high, it can force the model to make overly simplistic splits, resulting in underfitting.\n",
    "\n",
    "A higher Minimum Information Gain means that only splits that significantly reduce impurity are considered, which can help avoid splits that are not meaningful. If the threshold is too high, the tree might not split when it should, potentially leading to underfitting. Conversely, a very low threshold might allow splits that contribute little, increasing the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cab3dfa-39c4-4cb6-8ad6-8d03a60532b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_hyperparams_cv(X_train, y_train, num_classes,\n",
    "                            max_depth, min_samples_leaf, min_info_gain,\n",
    "                            cv=5, random_state=42):\n",
    "    # Evaluate the average F1-score for the given hyperparameter combination on the training set using StratifiedKFold.\n",
    "    cv_scores = []\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        tree = DecisionTreePrePruned(max_depth=max_depth,\n",
    "                                     min_samples_leaf=min_samples_leaf,\n",
    "                                     min_info_gain=min_info_gain)\n",
    "        tree.fit(X_tr, y_tr)\n",
    "        y_pred = tree.predict(X_val)\n",
    "        score = f1_score(y_val, y_pred, average='macro')\n",
    "        cv_scores.append(score)\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "def grid_search_cv(X_train, y_train, hyperparams_grid, cv=5, random_state=42):\n",
    "    # Iterate over hyperparameter combinations and select the best set using cross-validation.\n",
    "    best_cv_score = -np.inf\n",
    "    best_params = None\n",
    "    all_scores = []\n",
    "    \n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    for max_depth in hyperparams_grid['max_depth']:\n",
    "        for min_samples_leaf in hyperparams_grid['min_samples_leaf']:\n",
    "            for min_info_gain in hyperparams_grid['min_info_gain']:\n",
    "                score = evaluate_hyperparams_cv(X_train, y_train, num_classes,\n",
    "                                                max_depth, min_samples_leaf, min_info_gain,\n",
    "                                                cv=cv, random_state=random_state)\n",
    "                all_scores.append((max_depth, min_samples_leaf, min_info_gain, score))\n",
    "                print(\"Hyperparameters: max_depth={}, min_samples_leaf={}, min_info_gain={}, F1-score={:.4f}\".format(max_depth, min_samples_leaf, min_info_gain, score))\n",
    "                if score > best_cv_score:\n",
    "                    best_cv_score = score\n",
    "                    best_params = (max_depth, min_samples_leaf, min_info_gain)\n",
    "    return best_params, best_cv_score, all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c7df323-d2e5-474f-aaa3-8b2424970584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: max_depth=3, min_samples_leaf=2, min_info_gain=1e-12, F1-score=0.1186\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=2, min_info_gain=1e-05, F1-score=0.1186\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=5, min_info_gain=1e-12, F1-score=0.1186\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=5, min_info_gain=1e-05, F1-score=0.1186\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=10, min_info_gain=1e-12, F1-score=0.1186\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=10, min_info_gain=1e-05, F1-score=0.1186\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=2, min_info_gain=1e-12, F1-score=0.6716\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=2, min_info_gain=1e-05, F1-score=0.6716\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=5, min_info_gain=1e-12, F1-score=0.6709\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=5, min_info_gain=1e-05, F1-score=0.6709\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=10, min_info_gain=1e-12, F1-score=0.6675\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=10, min_info_gain=1e-05, F1-score=0.6675\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=2, min_info_gain=1e-12, F1-score=0.8292\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=2, min_info_gain=1e-05, F1-score=0.8292\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=5, min_info_gain=1e-12, F1-score=0.8201\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=5, min_info_gain=1e-05, F1-score=0.8201\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=10, min_info_gain=1e-12, F1-score=0.7861\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=10, min_info_gain=1e-05, F1-score=0.7861\n",
      "Best Hyperparameters for Letter Data Set: (11, 2, 1e-12)\n",
      "Best F1-score for Letter Data Set: 0.8292043698184045\n"
     ]
    }
   ],
   "source": [
    "# Train on letter dataset\n",
    "# Define hyperparameter grid\n",
    "hyperparams_grid = {\n",
    "    'max_depth': [3, 7, 11],\n",
    "    'min_samples_leaf': [2, 5, 10],\n",
    "    'min_info_gain': [1e-12, 1e-5]\n",
    "}\n",
    "\n",
    "# Use cross-validation on the training set to select hyperparameters\n",
    "letter_best_params, letter_best_cv_score, letter_all_scores = grid_search_cv(\n",
    "    letter_X_train, letter_y_train, hyperparams_grid, cv=5, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Best Hyperparameters for Letter Data Set:\", letter_best_params)\n",
    "print(\"Best F1-score for Letter Data Set:\", letter_best_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65268878-84ef-4796-b3f3-be459192df61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: max_depth=3, min_samples_leaf=2, min_info_gain=1e-12, F1-score=0.7421\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=2, min_info_gain=1e-05, F1-score=0.7421\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=5, min_info_gain=1e-12, F1-score=0.7421\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=5, min_info_gain=1e-05, F1-score=0.7421\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=10, min_info_gain=1e-12, F1-score=0.7421\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=10, min_info_gain=1e-05, F1-score=0.7421\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=2, min_info_gain=1e-12, F1-score=0.7744\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=2, min_info_gain=1e-05, F1-score=0.7744\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=5, min_info_gain=1e-12, F1-score=0.7738\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=5, min_info_gain=1e-05, F1-score=0.7738\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=10, min_info_gain=1e-12, F1-score=0.7736\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=10, min_info_gain=1e-05, F1-score=0.7736\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=2, min_info_gain=1e-12, F1-score=0.7861\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=2, min_info_gain=1e-05, F1-score=0.7861\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=5, min_info_gain=1e-12, F1-score=0.7853\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=5, min_info_gain=1e-05, F1-score=0.7853\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=10, min_info_gain=1e-12, F1-score=0.7856\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=10, min_info_gain=1e-05, F1-score=0.7856\n",
      "Best Hyperparameters for Adult Data Set: (11, 2, 1e-12)\n",
      "Best F1-score for Adult Data Set: 0.7861040138492207\n"
     ]
    }
   ],
   "source": [
    "# Train on adult dataset\n",
    "# Use cross-validation on the training set to select hyperparameters\n",
    "adult_best_params, adult_best_cv_score, adult_all_scores = grid_search_cv(\n",
    "    adult_X_train, adult_y_train, hyperparams_grid, cv=5, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Best Hyperparameters for Adult Data Set:\", adult_best_params)\n",
    "print(\"Best F1-score for Adult Data Set:\", adult_best_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c5bca7f-2b1a-43b9-be22-bd290eb8e259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: max_depth=3, min_samples_leaf=2, min_info_gain=1e-12, F1-score=0.9557\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=2, min_info_gain=1e-05, F1-score=0.9557\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=5, min_info_gain=1e-12, F1-score=0.9557\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=5, min_info_gain=1e-05, F1-score=0.9557\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=10, min_info_gain=1e-12, F1-score=0.9557\n",
      "Hyperparameters: max_depth=3, min_samples_leaf=10, min_info_gain=1e-05, F1-score=0.9557\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=2, min_info_gain=1e-12, F1-score=1.0000\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=2, min_info_gain=1e-05, F1-score=1.0000\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=5, min_info_gain=1e-12, F1-score=0.9991\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=5, min_info_gain=1e-05, F1-score=0.9991\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=10, min_info_gain=1e-12, F1-score=0.9988\n",
      "Hyperparameters: max_depth=7, min_samples_leaf=10, min_info_gain=1e-05, F1-score=0.9988\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=2, min_info_gain=1e-12, F1-score=1.0000\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=2, min_info_gain=1e-05, F1-score=1.0000\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=5, min_info_gain=1e-12, F1-score=0.9991\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=5, min_info_gain=1e-05, F1-score=0.9991\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=10, min_info_gain=1e-12, F1-score=0.9988\n",
      "Hyperparameters: max_depth=11, min_samples_leaf=10, min_info_gain=1e-05, F1-score=0.9988\n",
      "Best Hyperparameters for Mushroom Data Set: (7, 2, 1e-12)\n",
      "Best F1-score for Mushroom Data Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# train on mushroom dataset\n",
    "# Use cross-validation on the training set to select hyperparameters\n",
    "mushroom_best_params, mushroom_best_cv_score, mushroom_all_scores = grid_search_cv(\n",
    "    mushroom_X_train, mushroom_y_train, hyperparams_grid, cv=5, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Best Hyperparameters for Mushroom Data Set:\", mushroom_best_params)\n",
    "print(\"Best F1-score for Mushroom Data Set:\", mushroom_best_cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c491c8-e5e8-4bce-ad11-03cb608c4008",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate model performance using F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2dc3ad9e-3668-4ab6-aafb-e8be27eeb051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model F1-score on Letter Data Set: 0.8442818621618978\n",
      "Final Model F1-score on Adult Data Set: 0.795858979627702\n",
      "Final Model F1-score on Mushroom Data Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Use the given model to make predictions on the test set and return the macro F1-score as the evaluation metric.\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    score = f1_score(y_test, y_test_pred, average='macro')\n",
    "    return score\n",
    "\n",
    "# Train the final model on the entire training set for Letter data\n",
    "letter_final_model = DecisionTreePrePruned(\n",
    "    max_depth=letter_best_params[0],\n",
    "    min_samples_leaf=letter_best_params[1],\n",
    "    min_info_gain=letter_best_params[2]\n",
    ")\n",
    "letter_final_model.fit(letter_X_train, letter_y_train)\n",
    "\n",
    "# Evaluate the final model's performance on the test set for Letter data\n",
    "letter_test_f1 = evaluate_model(letter_final_model, letter_X_test, letter_y_test)\n",
    "print(\"Final Model F1-score on Letter Data Set:\", letter_test_f1)\n",
    "\n",
    "# Train the final model on the entire training set for Adult data\n",
    "adult_final_model = DecisionTreePrePruned(\n",
    "    max_depth=adult_best_params[0],\n",
    "    min_samples_leaf=adult_best_params[1],\n",
    "    min_info_gain=adult_best_params[2]\n",
    ")\n",
    "adult_final_model.fit(adult_X_train, adult_y_train)\n",
    "\n",
    "# Evaluate the final model's performance on the test set for Adult data\n",
    "adult_test_f1 = evaluate_model(adult_final_model, adult_X_test, adult_y_test)\n",
    "print(\"Final Model F1-score on Adult Data Set:\", adult_test_f1)\n",
    "\n",
    "# Train the final model on the entire training set for Mushroom data\n",
    "mushroom_final_model = DecisionTreePrePruned(\n",
    "    max_depth=mushroom_best_params[0],\n",
    "    min_samples_leaf=mushroom_best_params[1],\n",
    "    min_info_gain=mushroom_best_params[2]\n",
    ")\n",
    "mushroom_final_model.fit(mushroom_X_train, mushroom_y_train)\n",
    "\n",
    "# Evaluate the final model's performance on the test set for Mushroom data\n",
    "mushroom_test_f1 = evaluate_model(mushroom_final_model, mushroom_X_test, mushroom_y_test)\n",
    "print(\"Final Model F1-score on Mushroom Data Set:\", mushroom_test_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6559832e-8b1b-4190-9c00-d10306f95782",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "## 4.1 Compare the three models (stump, unpruned, pruned) on each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "236a16ac-714f-4217-affb-a4dccfdf8176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stump Model Test F1-score on Letter Data Set: 0.010393792222054749\n",
      "Unpruned Tree Model Test F1-score on Letter Data Set: 0.8737667040576732\n",
      "Pruned Model F1-score on Letter Data Set: 0.8442818621618978\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the F1-score for the stump model on the Letter dataset test set\n",
    "letter_stump_f1 = evaluate_model(stump_letter, letter_X_test, letter_y_test)\n",
    "# Evaluate the F1-score for the unpruned decision tree model on the Letter dataset test set\n",
    "letter_unpruned_f1 = evaluate_model(letter_UnprunedTree, letter_X_test, letter_y_test)\n",
    "\n",
    "print(\"Stump Model Test F1-score on Letter Data Set:\", letter_stump_f1)\n",
    "print(\"Unpruned Tree Model Test F1-score on Letter Data Set:\", letter_unpruned_f1)\n",
    "print(\"Pruned Model F1-score on Letter Data Set:\", letter_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c13d5cb-6185-4b3d-b378-50e8496ad271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stump Model Test F1-score on Adult Data Set: 0.43203488372093024\n",
      "Unpruned Tree Model Test F1-score on Adult Data Set: 0.7503145013068309\n",
      "Pruned Model F1-score on Adult Data Set: 0.795858979627702\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the F1-score for the stump model on the Adult dataset test set\n",
    "adult_stump_f1 = evaluate_model(stump_adult, adult_X_test, adult_y_test)\n",
    "# Evaluate the F1-score for the unpruned decision tree model on the Adult dataset test set\n",
    "adult_unpruned_f1 = evaluate_model(adult_UnprunedTree, adult_X_test, adult_y_test)\n",
    "\n",
    "print(\"Stump Model Test F1-score on Adult Data Set:\", adult_stump_f1)\n",
    "print(\"Unpruned Tree Model Test F1-score on Adult Data Set:\", adult_unpruned_f1)\n",
    "print(\"Pruned Model F1-score on Adult Data Set:\", adult_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40c45e54-8ddf-4860-96bd-a738db568d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stump Model Test F1-score on Mushroom Data Set: 0.7153695070361736\n",
      "Unpruned Tree Model Test F1-score on Mushroom Data Set: 1.0\n",
      "Pruned Model F1-score on Mushroom Data Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the F1-score for the stump model on the Mushroom dataset test set\n",
    "mushroom_stump_f1 = evaluate_model(stump_mushroom, mushroom_X_test, mushroom_y_test)\n",
    "# Evaluate the F1-score for the unpruned decision tree model on the Mushroom dataset test set\n",
    "mushroom_unpruned_f1 = evaluate_model(mushroom_UnprunedTree, mushroom_X_test, mushroom_y_test)\n",
    "\n",
    "print(\"Stump Model Test F1-score on Mushroom Data Set:\", mushroom_stump_f1)\n",
    "print(\"Unpruned Tree Model Test F1-score on Mushroom Data Set:\", mushroom_unpruned_f1)\n",
    "print(\"Pruned Model F1-score on Mushroom Data Set:\", mushroom_test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693a494-9b7f-4efa-83f6-ec6785d137c8",
   "metadata": {},
   "source": [
    "For the Letter dataset, using a more complex tree (either unpruned or pruned) is essential to achieve high performance. However, for the Adult and Mushroom datasets, the single-split model (stump) performs just as well as the more complex trees, suggesting that additional splits do not provide further benefit on these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c2e57-47c4-4e4a-9b90-c5bce4cb77b8",
   "metadata": {},
   "source": [
    "## 4.2 Report the p-value and discuss whether performance differences are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3154038-f58b-4caf-97fc-cedc2e6fa362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03d46708-25ca-4198-8b9d-47f6df0a18c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value for Letter Dataset (Stump vs Unpruned): 1.2429676145940019e-09\n",
      "P-value for Letter Dataset (Stump vs Pruned): 5.276150422360317e-11\n",
      "P-value for Letter Dataset (Unpruned vs Pruned): 4.048336691543988e-08\n",
      "P-value for Adult Dataset (Stump vs Unpruned): 1.5509446033995704e-07\n",
      "P-value for Adult Dataset (Stump vs Pruned): 5.435114264374745e-09\n",
      "P-value for Adult Dataset (Unpruned vs Pruned): 0.017951708709013203\n",
      "P-value for Mushroom Dataset (Stump vs Unpruned): 9.043144596544742e-05\n",
      "P-value for Mushroom Dataset (Stump vs Pruned): 0.0001072140825207062\n",
      "P-value for Mushroom Dataset (Unpruned vs Pruned): 8.166603239255997e-06\n"
     ]
    }
   ],
   "source": [
    "def evaluate_hyperparams_cvscores(X_train, y_train, num_classes,\n",
    "                            max_depth, min_samples_leaf, min_info_gain,\n",
    "                            cv=5, random_state=42):\n",
    "    # Evaluate the average F1-score for the given hyperparameter combination on the training set using StratifiedKFold.\n",
    "    cv_scores = []\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        tree = DecisionTreePrePruned(max_depth=max_depth,\n",
    "                                     min_samples_leaf=min_samples_leaf,\n",
    "                                     min_info_gain=min_info_gain)\n",
    "        tree.fit(X_tr, y_tr)\n",
    "        y_pred = tree.predict(X_val)\n",
    "        score = f1_score(y_val, y_pred, average='macro')\n",
    "        cv_scores.append(score)\n",
    "    return cv_scores\n",
    "    \n",
    "# Evaluate the models using cross-validation (CV) and store the F1-scores for each model\n",
    "num_classes1 = len(np.unique(letter_X_train))\n",
    "letter_stump_f1 = evaluate_hyperparams_cvscores(letter_X_train, letter_y_train, num_classes=num_classes1, max_depth=1, min_samples_leaf=1, min_info_gain=0.0)\n",
    "letter_unpruned_f1 = evaluate_hyperparams_cvscores(letter_X_train, letter_y_train, num_classes=num_classes1, max_depth=None, min_samples_leaf=1, min_info_gain=0.0)\n",
    "letter_pruned_f1 = evaluate_hyperparams_cvscores(letter_X_train, letter_y_train, num_classes=num_classes1, max_depth=5, min_samples_leaf=5, min_info_gain=0.05)\n",
    "\n",
    "num_classes2 = len(np.unique(adult_X_train))\n",
    "adult_stump_f1 = evaluate_hyperparams_cvscores(adult_X_train, adult_y_train, num_classes=num_classes2, max_depth=1, min_samples_leaf=1, min_info_gain=0.0)\n",
    "adult_unpruned_f1 = evaluate_hyperparams_cvscores(adult_X_train, adult_y_train, num_classes=num_classes2, max_depth=None, min_samples_leaf=1, min_info_gain=0.0)\n",
    "adult_pruned_f1 = evaluate_hyperparams_cvscores(adult_X_train, adult_y_train, num_classes=num_classes2, max_depth=5, min_samples_leaf=5, min_info_gain=0.05)\n",
    "\n",
    "num_classes3 = len(np.unique(mushroom_X_train))\n",
    "mushroom_stump_f1 = evaluate_hyperparams_cvscores(mushroom_X_train, mushroom_y_train, num_classes=num_classes3, max_depth=1, min_samples_leaf=1, min_info_gain=0.0)\n",
    "mushroom_unpruned_f1 = evaluate_hyperparams_cvscores(mushroom_X_train, mushroom_y_train, num_classes=num_classes3, max_depth=None, min_samples_leaf=1, min_info_gain=0.0)\n",
    "mushroom_pruned_f1 = evaluate_hyperparams_cvscores(mushroom_X_train, mushroom_y_train, num_classes=num_classes3, max_depth=5, min_samples_leaf=5, min_info_gain=0.05)\n",
    "\n",
    "# Calculate the p-values using paired t-tests\n",
    "p_letter_stump_vs_unpruned = ttest_rel(letter_stump_f1, letter_unpruned_f1).pvalue\n",
    "p_letter_stump_vs_pruned = ttest_rel(letter_stump_f1, letter_pruned_f1).pvalue\n",
    "p_letter_unpruned_vs_pruned = ttest_rel(letter_unpruned_f1, letter_pruned_f1).pvalue\n",
    "\n",
    "p_adult_stump_vs_unpruned = ttest_rel(adult_stump_f1, adult_unpruned_f1).pvalue\n",
    "p_adult_stump_vs_pruned = ttest_rel(adult_stump_f1, adult_pruned_f1).pvalue\n",
    "p_adult_unpruned_vs_pruned = ttest_rel(adult_unpruned_f1, adult_pruned_f1).pvalue\n",
    "\n",
    "p_mushroom_stump_vs_unpruned = ttest_rel(mushroom_stump_f1, mushroom_unpruned_f1).pvalue\n",
    "p_mushroom_stump_vs_pruned = ttest_rel(mushroom_stump_f1, mushroom_pruned_f1).pvalue\n",
    "p_mushroom_unpruned_vs_pruned = ttest_rel(mushroom_unpruned_f1, mushroom_pruned_f1).pvalue\n",
    "\n",
    "# Print p-values for each comparison\n",
    "print(\"P-value for Letter Dataset (Stump vs Unpruned):\", p_letter_stump_vs_unpruned)\n",
    "print(\"P-value for Letter Dataset (Stump vs Pruned):\", p_letter_stump_vs_pruned)\n",
    "print(\"P-value for Letter Dataset (Unpruned vs Pruned):\", p_letter_unpruned_vs_pruned)\n",
    "\n",
    "print(\"P-value for Adult Dataset (Stump vs Unpruned):\", p_adult_stump_vs_unpruned)\n",
    "print(\"P-value for Adult Dataset (Stump vs Pruned):\", p_adult_stump_vs_pruned)\n",
    "print(\"P-value for Adult Dataset (Unpruned vs Pruned):\", p_adult_unpruned_vs_pruned)\n",
    "\n",
    "print(\"P-value for Mushroom Dataset (Stump vs Unpruned):\", p_mushroom_stump_vs_unpruned)\n",
    "print(\"P-value for Mushroom Dataset (Stump vs Pruned):\", p_mushroom_stump_vs_pruned)\n",
    "print(\"P-value for Mushroom Dataset (Unpruned vs Pruned):\", p_mushroom_unpruned_vs_pruned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1bddd8-d12d-42bf-b37b-b99d3d184a8b",
   "metadata": {},
   "source": [
    "On the Letter dataset, the p-values among the three models are extremely small (all below 10⁻⁸), indicating that the differences in F1-scores among the decision stump, fully grown tree, and pruned tree are highly significant. On the Mushroom dataset, the p-values are all below 10⁻⁴, also demonstrating significant differences. However, on the Adult dataset, although the p-values between the decision stump and the other models are very low (approximately 1.55e-07 and 5.44e-09, respectively), the p-value between the fully grown tree and the pruned tree is 0.01795. While this value is still below the 0.05 threshold and thus indicates a statistically significant difference, it is the highest among the comparisons, suggesting that the performance difference between the fully grown and pruned trees is the smallest. This implies that, on the Adult dataset, pruning has a relatively mild impact on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891c1c9-d7cc-451f-93cf-9510d1e20802",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "## 5.1 Analyze the importance of features in your trained decision tree models (stump, unpruned, pruned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b169b95-2ff9-4513-93bc-1c22dba9c34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Letter Dataset==\n",
      "The most important feature for the stump model: y-ege, with information gain: 0.3957973075960277\n",
      "Top 3 important features for unpruned tree: [('x-box', 302.1417690072275), ('y-box', 163.28959786556084), ('y2bar', 75.7692431351547)]\n",
      "Top 3 important features for pruned tree: [('x-box', 302.1417690072275), ('y-box', 163.28959786556084), ('y2bar', 75.7692431351547)]\n",
      "\n",
      "==Adult Dataset==\n",
      "The most important feature for the stump model: relationship, with information gain: 0.11694712708808075\n",
      "Top 3 important features for unpruned tree: [('fnlwgt', 713.9163353899717), ('age', 595.7557824283717), ('workclass', 116.97846362244614)]\n",
      "Top 3 important features for pruned tree: [('fnlwgt', 713.9163353899717), ('age', 595.7557824283717), ('workclass', 116.97846362244614)]\n",
      "\n",
      "==Mushroom Dataset==\n",
      "The most important feature for the stump model: gill-color, with information gain: 0.2657134712601421\n",
      "Top 3 important features for unpruned tree: [('odor', 1.1567583262185512), ('spore-print-color', 0.8805830434183446), ('bruises', 0.7177418091683018)]\n",
      "Top 3 important features for pruned tree: [('odor', 1.1567583262185512), ('spore-print-color', 0.8805830434183446), ('bruises', 0.7177418091683018)]\n"
     ]
    }
   ],
   "source": [
    "def get_feature_importance(node, importance_dict=None):\n",
    "    # Recursively traverse the decision tree nodes and accumulate the information gain contribution of each feature from all internal nodes.\n",
    "    if importance_dict is None:\n",
    "        importance_dict = {}\n",
    "        \n",
    "    # If the current node is a leaf node, it does not contribute\n",
    "    if node.is_leaf:\n",
    "        return importance_dict\n",
    "\n",
    "    # Accumulate the contribution of the current node\n",
    "    f_idx = node.feature_idx\n",
    "    ig = getattr(node, 'ig', 0)\n",
    "    if f_idx in importance_dict:\n",
    "        importance_dict[f_idx] += ig\n",
    "    else:\n",
    "        importance_dict[f_idx] = ig\n",
    "\n",
    "    # Recursively traverse the left and right subtrees\n",
    "    get_feature_importance(node.left_child, importance_dict)\n",
    "    get_feature_importance(node.right_child, importance_dict)\n",
    "    \n",
    "    return importance_dict\n",
    "\n",
    "def get_top_features(importance_dict, top_n=3, feature_names=None):\n",
    "    # Get the top_n features with the highest contribution based on the importance_dict,\n",
    "    # and return a list of tuples containing the feature names and their corresponding importance.\n",
    "    # Sort to get feature indices in descending order of contribution\n",
    "    sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_features = sorted_features[:top_n]\n",
    "    \n",
    "    return [(feature_names[idx], imp) for idx, imp in top_features]\n",
    "\n",
    "# \n",
    "print(\"==Letter Dataset==\")\n",
    "importance_dict2 = get_feature_importance(letter_UnprunedTree.root)\n",
    "top3_letter2 = get_top_features(importance_dict2, top_n=3, feature_names=letter_X.columns.tolist())  # letter_X.columns is the list of original feature names\n",
    "importance_dict3 = get_feature_importance(letter_PrunedTree.root)\n",
    "top3_letter3 = get_top_features(importance_dict3, top_n=3, feature_names=letter_X.columns.tolist())  # letter_X.columns is the list of original feature names\n",
    "stump_letter.print_stump_details()\n",
    "print(\"Top 3 important features for unpruned tree:\", top3_letter2)\n",
    "print(\"Top 3 important features for pruned tree:\", top3_letter3)\n",
    "\n",
    "print(\"\\n==Adult Dataset==\")\n",
    "importance_dict2 = get_feature_importance(adult_UnprunedTree.root)\n",
    "top3_adult2 = get_top_features(importance_dict2, top_n=3, feature_names=adult_X.columns.tolist())  # adult_X.columns is the list of original feature names\n",
    "importance_dict3 = get_feature_importance(adult_PrunedTree.root)\n",
    "top3_adult3 = get_top_features(importance_dict3, top_n=3, feature_names=adult_X.columns.tolist())  # adult_X.columns is the list of original feature names\n",
    "stump_adult.print_stump_details()\n",
    "print(\"Top 3 important features for unpruned tree:\", top3_adult2)\n",
    "print(\"Top 3 important features for pruned tree:\", top3_adult3)\n",
    "\n",
    "print(\"\\n==Mushroom Dataset==\")\n",
    "importance_dict2 = get_feature_importance(mushroom_UnprunedTree.root)\n",
    "top3_mushroom2 = get_top_features(importance_dict2, top_n=3, feature_names=mushroom_X.columns.tolist())  # mushroom_X.columns is the list of original feature names\n",
    "importance_dict3 = get_feature_importance(mushroom_PrunedTree.root)\n",
    "top3_mushroom3 = get_top_features(importance_dict3, top_n=3, feature_names=mushroom_X.columns.tolist())  # mushroom_X.columns is the list of original feature names\n",
    "stump_mushroom.print_stump_details()\n",
    "print(\"Top 3 important features for unpruned tree:\", top3_mushroom2)\n",
    "print(\"Top 3 important features for pruned tree:\", top3_mushroom3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18af91-ee29-476d-b0e2-872b79729f5f",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "## 6.1 Measure the runtime performance of each decision tree model (stump, unpruned, pruned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a95724fd-921e-4b22-bc55-33eb8ace506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Dataset     Stump   Unpruned     Pruned\n",
      "0    Letter  0.154769   3.385594   3.442885\n",
      "1     Adult  0.758650  18.834683  19.019174\n",
      "2  Mushroom  0.081170   0.287353   0.293396\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame containing runtime data for different datasets and models\n",
    "data = {\n",
    "    'Dataset': ['Letter', 'Adult', 'Mushroom'],\n",
    "    'Stump': [letter_runtime1, adult_runtime1, mushroom_runtime1],\n",
    "    'Unpruned': [letter_runtime2, adult_runtime2, mushroom_runtime2],\n",
    "    'Pruned': [letter_runtime3, adult_runtime3, mushroom_runtime3]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c876195-bb07-4f16-a26d-bd6cc59f69ac",
   "metadata": {},
   "source": [
    "The Stump model utilizes only a single split, resulting in extremely short training times. Regardless of the dataset, its training time is significantly lower than the other two more complex decision tree models. The Unpruned and Pruned models have very similar training times on most datasets because both need to traverse a large number of internal nodes to build a complex tree structure. The process of computing information gain and other metrics leads to a significant increase in training time.  \n",
    "\n",
    "The Adult dataset has the longest training time, likely due to its larger sample size or the more complex relationships among features. In contrast, the Mushroom dataset has the shortest training time because it is relatively small. As the dataset size (or data complexity) increases, training time shows a clear upward trend. This increase is particularly pronounced in the fully grown tree model, where training time rises more significantly due to the need for continuous splitting and information gain calculations to construct a deep tree structure.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd6fee-8084-4522-a755-4f2aa688d08a",
   "metadata": {},
   "source": [
    "## 6.2 Discuss trade-offs between accuracy and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04265c-3a3a-4734-bdf7-9c0ff5aaa429",
   "metadata": {},
   "source": [
    "Fully grown decision trees can capture a lot of details in the data, so they tend to have very high accuracy on the training set. However, they usually end up being very deep and huge, which means both training and prediction take much more time and computing power. On the other hand, decision stumps and pruned trees are much simpler, so they run faster and are better suited for large datasets. But the trade-off is that these simpler trees might miss some of the more complex patterns in the data, leading to lower accuracy. If we have limited computational resources, it’s better to choose a model that runs faster even if it sacrifices a bit of accuracy. Plus, pruning techniques can help prevent overfitting, making the model's performance on new data more stable, which helps you balance accuracy and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318cfbe8-ee24-4ec4-a541-e22b851e23aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "panel-cell-order": [
   "f6f89cbe-5fdb-4df1-a61d-ab38b9ae1024",
   "9c7b46eb-e314-4541-829d-7c38dbe2d131",
   "f58b644c-9242-48f8-aa37-6df51bbef883"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
